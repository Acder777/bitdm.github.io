---
layout: page
mathjax: true
permalink: /2018/projects/p07/midterm/
---

## 项目进展报告

### 数据获取及预处理

本项目采用阿里云天池大数据竞赛所提供的微博用户数据集。天池官方提供两个数据集：测试集和预测集。
- 训练集数据是用户在2015-02-01至2015-07-31时间段发表的微博数据。包括用户标识、博文标识、发表时间、发表一周后的转发数、发表一周后的评论数、发表一周后的赞数、博文内容。博文的全部信息都映射为一行数据，其中对用户做了一定抽样，获取了抽样用户半年的原创博文，对用户标记和博文标记做了加密 发博时间精确到天级别。对数据集简单分析可得到：训练数据共有122,5088条（仅三条内容有缺失值），涉及37251个用户，每个用户至少发一条博文，博文最多数量前十在4909~31015条。
- 测试集数据是用户2015-08-01至2015-08-31时间段内发表的微博数据，包括用户标识、博文标识、发表时间、博文内容，共有17,7923条。我们将会预测测试集中的微博发表一周后转发、评论、点赞的具体数值。

重新对数据进行处理，分别对训练集和预测集进行处理。
- 删除“无用”的用户。因为数据集中存在大量的“无用”的用户，虽然他们看起来比较活跃，会存在微博发表的活跃度特别高，但是对于这部分用户互动（转发、评论、点赞）数平均值为0，并没有与其他的用户产生交互。这类用户很可能属于“僵尸粉”用户，可以直接处理为最低的档位，不用参与模型的预测。对于“无用”的用户，我们可以设置了一些规则将他们过滤掉，比如发表微博数很大，但是总的互动数基本为0的用户。
- 找出测试集中用户没有历史数据的用户集合。找到需要预测的用户之后，我们从训练集中抽取出他们的历史数据，从而进一步发现有1214个用户在训练集中不存在历史数据。对于这部分用户我们有两个方面考虑，一个是直接将他们统一处理为最低的档位；另一方面是根据模型，这个模型是根据用户发表微博的特征维度来考虑，而不结合用户的特征。

### 数据分析与可视化

我们可以简单的统计下每条微博的转发数，然后做图可以看出绝大多数微博的转发数处于比较低的水平(100以内)，服从长尾分布。
- 所有用户总体发表博文一周后的转发数的可视化分布
![long-tailed distributions for all users](https://github.com/ChenDanLu/bitdm.github.io/blob/master/2018/projects/P07/images/figure_1.png)

- 某个用户发表的博文一周后转发数的可视化分布
![ingle user1](https://github.com/ChenDanLu/bitdm.github.io/blob/master/2018/projects/P07/images/figure_13.png)
![ingle user2](https://github.com/ChenDanLu/bitdm.github.io/blob/master/2018/projects/P07/images/figure_7.png)
![ingle user3](https://github.com/ChenDanLu/bitdm.github.io/blob/master/2018/projects/P07/images/figure_3.png)

### 模型选取

选用了两种模型进行实验。
- 第一种方法就是传统的搜索策略，搜索策略中，也进行两种实验对比：固定值的预测和非固定值的预测。  
非固定值预测：对于每个uid，我们首先得到它的（f_min，f_median，f_max），（c_min，c_median，c_max），（l_min，l_medain，l_max），然后：  
    1.固定c_median(评论数的中位数)和l_medain（点赞数的中位数），搜索<f_min，f_max>（转发的最小值和最大值）之间的前向值，这会导致（f_medain，c_medain，l_medain）更高的分数，如果存在多个得分相同的结果，我们选择f_medain附近的结果。如果不存在获得比（f_medain，c_medain，l_medain）更高分数的任何结果，则比我们选择forward = f_medain  
    2.通过相同的方法搜索评论数  
    3.通过相同的方法搜索点赞数  
固定值的预测：正在实验和调整中。

- 第二种方法就是神经网络的方法。用来模拟统计数据（min, median, max, mean）和真实的数据（转发数，评论数，点赞数）之间的关系。将每个Uid的统计数据作为输入，就是有f_min，f_median，f_max,f_mean,c_min，c_median，c_max,c_mean，l_min，l_medain，l_max，l_mean，12个数据作为输入，输出端是c_count,f_count,l_count。在拟合高次方程之间需要对数据进行预处理，由于37251条用户的统计数据中，大部分都是0，并且有21507的用户的数据信息低于10条，所以选了删掉一部分统计量比较少的用户统计数据，减少这些小数据对于模型的影响。损失函数采用的最小均方误差。采用梯度下降的方法。

### 挖掘实验的结果

非固定值预测的方法：score 40.94%  
神经网络的方法：目前还在调整阶段，预测结果不是很好 

### 存在的问题

1.搜索策略如果用固定值进行预测，预测的结果比较依赖于固定值。目前的结果如下：  

  predict   | score(%)
------------|----------
 0  0  0    | 34.10%
 1  0  0    | 29.23% 
 0  1  0    | 35.01%
 0  0  1    | 32.20%
 1  1  0    | 29.30%
 1  0  1    | 29.39%
 0  1  1    | 33.45%
 1  1  1    | 13.46%
 2  0  0    | 7.04%
 0  2  0    | 29.93%
 0  0  2    | 28.22%
 0  1  2    | 12.85%
   ……       |   …… 

可以看到设置不同的固定值得到的预测结果不一样，因此对于不同的数据，可能预测效果不是很好，正在考虑调整别的固定值就行测试。  
2.非固定值的搜索预测的方法，整体的效果不能进行改进了，预测的分数最好的状态只能达到40%，所以考虑有没有其他的方法能提高预测结果分数的。  
3.基于神经网络的方法回归出每个用户的统计特性和真实的值之间的关系。在训练的时候出现了不收敛的问题。  

### 下一步工作

后续正在准备从以下五个方向进行实验：  
- 对数据进行预处理  
- 对数据进行归一化  
- 调整我们的损失函数，找到更合适的损失函数  
- 重新思考数据之间的对应关系  
- 考虑用强化学习的方法进行实验
